core:
  seed: 42

environment:
  n_envs: 4
  deadline_round: 100
  domains: ['5', '6', '7', '9', '10', '11', '12',
            '13', '14', '21', '22', '23', '25', '26', '27', '28', '29',
            '30', '37', '38', '39' ]

  opponents: [
        "ConcederAgent",
        "BoulwareAgent",
        "HybridAgent",
        "CUHKAgent",
        "MICROAgent",
        "SAGAAgent",
        "NiceTitForTat",
        "IAMhaggler",
        "HardHeaded",
        "PonPokoAgent",
        ]

training:
  total_timesteps: 10000000
  # PPO rollout buffer size per update. n_steps is derived as:
  #   n_steps = rollout_buffer_size / n_envs
  # Must be divisible by n_envs.
  rollout_buffer_size: 2048
  batch_size: 128

  learning_rate: 5.0e-4   # SB3 PPO default
  n_epochs: 10            # SB3 PPO default
  gamma: 0.99             # SB3 PPO default
  clip_range: 0.2
  ent_coef: 0.001         # slightly higher exploration on target head
  target_kl: 0.01         # SB3 PPO default (disabled)

  # Feed-forward feature extractor before recurrent head.
  policy_hidden_sizes: [128, 128, 64]

  # RecurrentPPO (MlpLstmPolicy) defaults.
  lstm_hidden_size: 128
  n_lstm_layers: 1
  shared_lstm: false
  enable_critic_lstm: true

logging:
  log_level: "INFO"
  wandb:
    project: "negotiation-rl"
    entity: null
  checkpoint_freq: 100000  # save a model artifact every N timesteps
