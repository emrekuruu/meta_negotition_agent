{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55afc60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Create plots directory if it doesn't exist\n",
    "PLOTS_DIR = Path(\"plots\")\n",
    "PLOTS_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Plots will be saved to: {PLOTS_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f01d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Who\", \"Action\", \"Round\", \"AgentAUtility\", \"AgentBUtility\",\"NashDistance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d6cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"RMSE\", \"Spearman\", \"KendallTau\", \"Pearson\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e509334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models =['Classic Frequency Opponent Model',\"CUHK Frequency Opponent Model\",\"Bayesian Opponent Model\",\"Stepwise COMB Opponent Model\", \n",
    "         \"Expectation COMB Opponent Model\", \"Conflict-Based Opponent Model\", \"Frequency Window Opponent Model\"]\n",
    "\n",
    "# =============================================================================\n",
    "# COLOR SCHEME CONFIGURATION - Wong's Colorblind-Safe Palette\n",
    "# =============================================================================\n",
    "# Reference: Wong, B. (2011). Points of view: Color blindness. Nature Methods 8, 441.\n",
    "# This palette is widely used in Nature, Science, and other scientific journals.\n",
    "\n",
    "WONG_COLORS = {\n",
    "    'orange': '#E69F00',\n",
    "    'sky_blue': '#56B4E9', \n",
    "    'bluish_green': '#009E73',\n",
    "    'yellow': '#F0E442',\n",
    "    'blue': '#0072B2',\n",
    "    'vermillion': '#D55E00',\n",
    "    'reddish_purple': '#CC79A7',\n",
    "    'black': '#000000'\n",
    "}\n",
    "\n",
    "# Map each opponent model to a specific color (consistent across all plots)\n",
    "MODEL_COLORS = {\n",
    "    'Classic Frequency Opponent Model': WONG_COLORS['blue'],\n",
    "    'CUHK Frequency Opponent Model': WONG_COLORS['orange'],\n",
    "    'Bayesian Opponent Model': WONG_COLORS['bluish_green'],\n",
    "    'Stepwise COMB Opponent Model': WONG_COLORS['vermillion'],\n",
    "    'Expectation COMB Opponent Model': WONG_COLORS['reddish_purple'],\n",
    "    'Conflict-Based Opponent Model': WONG_COLORS['sky_blue'],\n",
    "    'Frequency Window Opponent Model': WONG_COLORS['yellow']\n",
    "}\n",
    "\n",
    "# Map each opponent model to a distinct geometric marker\n",
    "MODEL_MARKERS = {\n",
    "    'Classic Frequency Opponent Model': 'o',      # Circle\n",
    "    'CUHK Frequency Opponent Model': 's',         # Square\n",
    "    'Bayesian Opponent Model': '^',               # Triangle up\n",
    "    'Stepwise COMB Opponent Model': 'D',          # Diamond\n",
    "    'Expectation COMB Opponent Model': 'v',       # Triangle down\n",
    "    'Conflict-Based Opponent Model': 'p',         # Pentagon\n",
    "    'Frequency Window Opponent Model': 'X'        # X (filled)\n",
    "}\n",
    "\n",
    "def get_model_color(model_name):\n",
    "    \"\"\"Get the assigned color for a given opponent model.\"\"\"\n",
    "    return MODEL_COLORS.get(model_name, WONG_COLORS['black'])\n",
    "\n",
    "def get_model_marker(model_name):\n",
    "    \"\"\"Get the assigned marker for a given opponent model.\"\"\"\n",
    "    return MODEL_MARKERS.get(model_name, 'o')\n",
    "\n",
    "def get_model_colors_list():\n",
    "    \"\"\"Get list of colors in the same order as models list.\"\"\"\n",
    "    return [get_model_color(model) for model in models]\n",
    "\n",
    "def get_model_markers_list():\n",
    "    \"\"\"Get list of markers in the same order as models list.\"\"\"\n",
    "    return [get_model_marker(model) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becdf59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = ['MICROAgent','HybridAgent', 'BoulwareAgent', 'SAGAAgent', 'CUHKAgent',\n",
    "          \"ConcederAgent\",\"NiceTitForTat\", \"IAMhaggler\", \"PonPokoAgent\", \"HardHeaded\"]\n",
    "\n",
    "def show_color_scheme():\n",
    "    \"\"\"\n",
    "    Display the color scheme mapping for opponent models.\n",
    "    Useful reference for paper figures.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    y_positions = np.arange(len(models))\n",
    "    colors = get_model_colors_list()\n",
    "    model_labels = [\" \".join(model.split(\" \")[0:-2]) for model in models]\n",
    "    \n",
    "    # Create horizontal bars with model colors\n",
    "    bars = ax.barh(y_positions, [1] * len(models), color=colors, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels(model_labels, fontsize=12)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title(\"Opponent Model Color Scheme\\n(Wong's Colorblind-Safe Palette)\", \n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add color hex codes as text\n",
    "    for i, (bar, color) in enumerate(zip(bars, colors)):\n",
    "        ax.text(0.5, i, color.upper(), ha='center', va='center', \n",
    "               fontsize=10, fontweight='bold', color='white' if i in [0, 3] else 'black')\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf7e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_path = \"/home/ubuntu/negoformer/Negolog-RL/results/oracle/1000\"\n",
    "\n",
    "# =============================================================================\n",
    "# DOMAIN CATEGORIZATION - Load domain metadata\n",
    "# =============================================================================\n",
    "domains_metadata_path = \"../../domains/domains.xlsx\"\n",
    "\n",
    "def load_domain_categories():\n",
    "    \"\"\"\n",
    "    Load domain categorization from metadata file.\n",
    "    Returns dict with three category lists: size, opposition, balance.\n",
    "    Each contains tuples of (domain_name, value).\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(domains_metadata_path)\n",
    "    \n",
    "    # Convert DomainName to string to handle mixed types\n",
    "    if 'DomainName' in df.columns:\n",
    "        df['DomainName'] = df['DomainName'].astype(str)\n",
    "    \n",
    "    # Display structure for debugging\n",
    "    print(\"Domain metadata columns:\", df.columns.tolist())\n",
    "    print(f\"\\nLoaded {len(df)} domains\")\n",
    "    \n",
    "    # Detect column names (flexible to handle different naming conventions)\n",
    "    domain_col = None\n",
    "    size_col = None\n",
    "    opp_col = None\n",
    "    bal_col = None\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if 'domain' in col_lower or 'name' in col_lower:\n",
    "            domain_col = col\n",
    "        elif 'size' in col_lower:\n",
    "            size_col = col\n",
    "        elif 'opposition' in col_lower or 'opp' in col_lower:\n",
    "            opp_col = col\n",
    "        elif 'balance' in col_lower or 'bal' in col_lower:\n",
    "            bal_col = col\n",
    "    \n",
    "    print(f\"\\nDetected columns:\")\n",
    "    print(f\"  Domain: {domain_col}\")\n",
    "    print(f\"  Size: {size_col}\")\n",
    "    print(f\"  Opposition: {opp_col}\")\n",
    "    print(f\"  Balance: {bal_col}\")\n",
    "    \n",
    "    # Create categorized lists\n",
    "    domain_categories = {\n",
    "        'size': [],\n",
    "        'opposition': [],\n",
    "        'balance': []\n",
    "    }\n",
    "    \n",
    "    if domain_col:\n",
    "        if size_col:\n",
    "            domain_categories['size'] = list(zip(df[domain_col], df[size_col]))\n",
    "        if opp_col:\n",
    "            domain_categories['opposition'] = list(zip(df[domain_col], df[opp_col]))\n",
    "        if bal_col:\n",
    "            domain_categories['balance'] = list(zip(df[domain_col], df[bal_col]))\n",
    "    \n",
    "    return domain_categories, df\n",
    "\n",
    "def normalize_domain_name(name):\n",
    "    \"\"\"Normalize domain name for matching (lowercase, strip whitespace).\"\"\"\n",
    "    return str(name).lower().strip()\n",
    "\n",
    "def create_domain_lookup(domain_results):\n",
    "    \"\"\"\n",
    "    Create a lookup dictionary mapping normalized domain names to actual names.\n",
    "    \"\"\"\n",
    "    return {normalize_domain_name(name): name for name in domain_results.keys()}\n",
    "\n",
    "def get_domains_by_category(domain_results, category_type='size'):\n",
    "    \"\"\"\n",
    "    Get domains sorted by a specific category (size, opposition, or balance).\n",
    "    \n",
    "    Args:\n",
    "        domain_results: Results dict from compute_domain_results\n",
    "        category_type: 'size', 'opposition', or 'balance'\n",
    "    \n",
    "    Returns:\n",
    "        List of domain names sorted by the category value\n",
    "    \"\"\"\n",
    "    if domain_metadata_df is None:\n",
    "        return sorted(domain_results.keys())\n",
    "    \n",
    "    category_data = domain_categories.get(category_type, [])\n",
    "    \n",
    "    if not category_data:\n",
    "        return sorted(domain_results.keys())\n",
    "    \n",
    "    # Create lookup for case-insensitive matching\n",
    "    domain_lookup = create_domain_lookup(domain_results)\n",
    "    \n",
    "    # Filter to only domains that exist in results (case-insensitive)\n",
    "    available_domains = []\n",
    "    for name, val in category_data:\n",
    "        normalized_name = normalize_domain_name(name)\n",
    "        if normalized_name in domain_lookup:\n",
    "            actual_name = domain_lookup[normalized_name]\n",
    "            available_domains.append((actual_name, val))\n",
    "    \n",
    "    # Sort by value\n",
    "    available_domains.sort(key=lambda x: x[1])\n",
    "    \n",
    "    return [name for name, val in available_domains]\n",
    "\n",
    "def debug_domain_matching(domain_results):\n",
    "    \"\"\"\n",
    "    Print debugging information about domain name matching.\n",
    "    Helps identify why domains aren't matching between metadata and results.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DOMAIN MATCHING DEBUG\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if domain_metadata_df is None:\n",
    "        print(\"No domain metadata loaded!\")\n",
    "        return\n",
    "    \n",
    "    # Get domain names from both sources (convert to strings)\n",
    "    if 'DomainName' in domain_metadata_df.columns:\n",
    "        metadata_domains = set(str(d) for d in domain_metadata_df['DomainName'].values)\n",
    "    else:\n",
    "        metadata_domains = set()\n",
    "    \n",
    "    results_domains = set(str(d) for d in domain_results.keys())\n",
    "    \n",
    "    print(f\"\\nDomains in metadata: {len(metadata_domains)}\")\n",
    "    print(f\"Domains in results: {len(results_domains)}\")\n",
    "    \n",
    "    # Show first few from each\n",
    "    print(f\"\\nFirst 5 metadata domains: {sorted(list(metadata_domains))[:5]}\")\n",
    "    print(f\"First 5 results domains: {sorted(list(results_domains))[:5]}\")\n",
    "    \n",
    "    # Check for exact matches\n",
    "    exact_matches = metadata_domains & results_domains\n",
    "    print(f\"\\nExact matches: {len(exact_matches)}\")\n",
    "    \n",
    "    # Check for case-insensitive matches\n",
    "    metadata_normalized = {normalize_domain_name(d): d for d in metadata_domains}\n",
    "    results_normalized = {normalize_domain_name(d): d for d in results_domains}\n",
    "    \n",
    "    case_insensitive_matches = set(metadata_normalized.keys()) & set(results_normalized.keys())\n",
    "    print(f\"Case-insensitive matches: {len(case_insensitive_matches)}\")\n",
    "    \n",
    "    if len(case_insensitive_matches) > 0:\n",
    "        print(\"\\nExample matches:\")\n",
    "        for norm in list(sorted(case_insensitive_matches))[:5]:\n",
    "            print(f\"  Metadata: '{metadata_normalized[norm]}' <-> Results: '{results_normalized[norm]}'\")\n",
    "    \n",
    "    # Show mismatches\n",
    "    metadata_only = metadata_domains - results_domains\n",
    "    results_only = results_domains - metadata_domains\n",
    "    \n",
    "    if metadata_only:\n",
    "        print(f\"\\nIn metadata but NOT in results ({len(metadata_only)}): {sorted(list(metadata_only))[:5]}\")\n",
    "    if results_only:\n",
    "        print(f\"\\nIn results but NOT in metadata ({len(results_only)}): {sorted(list(results_only))[:5]}\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Load once at startup\n",
    "try:\n",
    "    domain_categories, domain_metadata_df = load_domain_categories()\n",
    "except Exception as e:\n",
    "    print(f\"Could not load domain metadata: {e}\")\n",
    "    domain_metadata_df = None\n",
    "    domain_categories = {'size': [], 'opposition': [], 'balance': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f777821",
   "metadata": {},
   "source": [
    "# Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3188ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CORE UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def get_domains(base_dir):\n",
    "    domains = set()\n",
    "    for domain in os.listdir(base_dir):\n",
    "        domain_path = os.path.join(base_dir, domain)\n",
    "        if os.path.isdir(domain_path):\n",
    "            domains.add(domain)\n",
    "    return domains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347aef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# UNIFIED SESSION LOADING - Read each session file ONCE\n",
    "# =============================================================================\n",
    "\n",
    "def load_session_data(file_path):\n",
    "    \"\"\"\n",
    "    Load all data from a session file in one pass.\n",
    "    Returns a dictionary with all sheets and metadata cached.\n",
    "    \"\"\"\n",
    "    result = {'filename': file_path.name, 'models': {}}\n",
    "    \n",
    "    try:\n",
    "        # Parse agent names from filename: AgentA_AgentB_DomainX_ProcessY.xlsx\n",
    "        parts = file_path.name.replace('.xlsx', '').split('_')\n",
    "        result['agent_a'] = parts[0]\n",
    "        result['agent_b'] = parts[1]\n",
    "        \n",
    "        # Read Session sheet\n",
    "        session_df = pd.read_excel(file_path, sheet_name=\"Session\")\n",
    "        result['session_df'] = session_df\n",
    "        \n",
    "        # Find Accept row index\n",
    "        accept_mask = session_df['Action'] == 'Accept'\n",
    "        accept_idx = session_df[accept_mask].index\n",
    "        result['accept_row_idx'] = accept_idx[0] if len(accept_idx) > 0 else len(session_df) - 1\n",
    "        \n",
    "        # Read all model sheets\n",
    "        for model in models:\n",
    "            try:\n",
    "                model_df = pd.read_excel(file_path, sheet_name=model)\n",
    "                result['models'][model] = {\n",
    "                    'full_df': model_df,\n",
    "                    'last_row': model_df.iloc[-1]  # Cache last row\n",
    "                }\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return result\n",
    "\n",
    "def load_all_session_data(base_dir, n_jobs=32):\n",
    "    \"\"\"\n",
    "    Load ALL session files across all domains ONCE.\n",
    "    Returns: {domain: [SessionData, SessionData, ...]}\n",
    "    \"\"\"\n",
    "    all_data = {}\n",
    "    domains = get_domains(base_dir)\n",
    "    \n",
    "    print(f\"\\nLoading all session data from {len(domains)} domains...\")\n",
    "    \n",
    "    for domain in domains:\n",
    "        try:\n",
    "            sessions_dir = Path(base_dir) / domain / \"sessions\"\n",
    "            if sessions_dir.exists():\n",
    "                files = list(sessions_dir.iterdir())\n",
    "                \n",
    "                # Parallel load all sessions for this domain\n",
    "                session_data_list = Parallel(n_jobs=n_jobs)(\n",
    "                    delayed(load_session_data)(file)\n",
    "                    for file in tqdm(files, desc=f\"Loading {domain}\")\n",
    "                )\n",
    "                \n",
    "                # Filter out None results\n",
    "                session_data_list = [data for data in session_data_list if data is not None]\n",
    "                \n",
    "                all_data[domain] = session_data_list\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading domain {domain}: {e}\")\n",
    "    \n",
    "    total_sessions = sum(len(sessions) for sessions in all_data.values())\n",
    "    print(f\"\\nLoaded {total_sessions} total sessions across {len(all_data)} domains\")\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef36bc7",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# EXTRACT FUNCTIONS - Process pre-loaded session data (no file I/O)\n# =============================================================================\n\ndef extract_domain_results(session_data, metric_prefix=\"\"):\n    \"\"\"\n    Extract final metrics from a pre-loaded session.\n    Uses cached last_row from each model.\n    \"\"\"\n    results = {}\n    \n    for model in models:\n        if model in session_data['models']:\n            results[model] = {}\n            last_row = session_data['models'][model]['last_row']\n            \n            for metric in metrics:\n                column_name = f\"{metric_prefix}{metric}\"\n                if column_name in last_row.index:\n                    results[model][metric] = last_row[column_name]\n    \n    return results\n\ndef extract_round_by_round(session_data, metric_prefix=\"\"):\n    \"\"\"\n    Extract round-by-round metrics from a pre-loaded session.\n    Only returns data for rounds that actually exist - NO NaN filling.\n    \"\"\"\n    results = {}\n    \n    if 'session_df' not in session_data:\n        return results\n    \n    session_df = session_data['session_df']\n    if 'Round' not in session_df.columns:\n        return results\n    \n    accept_row_idx = session_data.get('accept_row_idx', len(session_df) - 1)\n    \n    for model in models:\n        if model not in session_data['models']:\n            continue\n            \n        model_df = session_data['models'][model]['full_df']\n        \n        # Slice to accept row\n        slice_end = min(accept_row_idx + 1, len(model_df), len(session_df))\n        model_df_slice = model_df.iloc[:slice_end].reset_index(drop=True)\n        session_df_slice = session_df.iloc[:slice_end].reset_index(drop=True)\n        \n        model_df_slice['Round'] = session_df_slice['Round']\n        results[model] = {}\n        \n        for metric in metrics:\n            column_name = f\"{metric_prefix}{metric}\"\n            if column_name not in model_df_slice.columns:\n                continue\n            \n            # Group by round and compute mean - returns ONLY rounds that exist\n            round_means = model_df_slice.groupby('Round')[column_name].mean()\n            \n            # Just convert to dict: {round_number: value} - no NaN filling!\n            results[model][metric] = dict(round_means)\n    \n    return results\n\ndef extract_opponent_analysis(session_data, metric_prefix=\"\"):\n    \"\"\"\n    Extract per-agent metrics from a pre-loaded session.\n    Uses cached last_row and pre-parsed agent names.\n    \n    Args:\n        session_data: Pre-loaded session data\n        metric_prefix: Prefix for metric columns (e.g., \"Overall_\" for Pareto metrics)\n    \"\"\"\n    results = {}\n    \n    agent_a = session_data['agent_a']\n    agent_b = session_data['agent_b']\n    \n    if agent_a not in results:\n        results[agent_a] = {}\n    if agent_b not in results:\n        results[agent_b] = {}\n    \n    for model in models:\n        if model not in session_data['models']:\n            continue\n            \n        last_row = session_data['models'][model]['last_row']\n        \n        if model not in results[agent_a]:\n            results[agent_a][model] = {}\n        if model not in results[agent_b]:\n            results[agent_b][model] = {}\n        \n        # Extract metrics for Agent A\n        for metric in metrics:\n            # Try with _A suffix first, then without\n            metric_col_a = f\"{metric_prefix}{metric}_A\"\n            if metric_col_a not in last_row.index:\n                metric_col_a = f\"{metric_prefix}{metric}\"\n            if metric_col_a in last_row.index:\n                results[agent_a][model][metric] = last_row[metric_col_a]\n        \n        # Extract metrics for Agent B\n        for metric in metrics:\n            # Try with _B suffix first, then without\n            metric_col_b = f\"{metric_prefix}{metric}_B\"\n            if metric_col_b not in last_row.index:\n                metric_col_b = f\"{metric_prefix}{metric}\"\n            if metric_col_b in last_row.index:\n                results[agent_b][model][metric] = last_row[metric_col_b]\n    \n    return results\ndef extract_box_rmse(session_data):\n    \"\"\"\n    Safe version: Checks if 'session_df' exists before accessing it.\n    \"\"\"\n    results = {}\n    \n    # SAFETY CHECK: Return empty dict if key is missing\n    if 'session_df' not in session_data:\n        return results\n\n    session_df = session_data['session_df']\n    if 'Round' not in session_df.columns:\n        return results\n    \n    # Safety Check: Accept row might be missing\n    accept_row_idx = session_data.get('accept_row_idx', len(session_df))\n    max_round = int(session_df['Round'].max())\n    \n    for model in models:\n        # Safety Check\n        if model not in session_data['models']:\n            continue\n            \n        model_df = session_data['models'][model]['full_df']\n        \n        if 'BoxCount_A' not in model_df.columns or 'BoxCount_B' not in model_df.columns:\n            continue\n        \n        try:\n            box_count_a = int(model_df['BoxCount_A'].iloc[0])\n            box_count_b = int(model_df['BoxCount_B'].iloc[0])\n        except:\n            continue\n            \n        common_boxes = min(box_count_a, box_count_b)\n        \n        if common_boxes == 0:\n            continue\n        \n        model_data = {box_idx: [[] for _ in range(max_round + 1)] for box_idx in range(common_boxes)}\n        \n        for row_idx in range(min(len(session_df), len(model_df), accept_row_idx + 1)):\n            if session_df.iloc[row_idx][\"Action\"] == \"Accept\":\n                break\n            \n            try:\n                round_num = int(session_df.iloc[row_idx][\"Round\"])\n            except:\n                continue\n            \n            for box_idx in range(common_boxes):\n                rmse_a_col = f'Box{box_idx}_RMSE_A'\n                rmse_b_col = f'Box{box_idx}_RMSE_B'\n                \n                if rmse_a_col in model_df.columns and rmse_b_col in model_df.columns:\n                    rmse_a = model_df.iloc[row_idx][rmse_a_col]\n                    rmse_b = model_df.iloc[row_idx][rmse_b_col]\n                    \n                    if pd.notna(rmse_a) and pd.notna(rmse_b):\n                        avg_rmse = (rmse_a + rmse_b) / 2\n                        model_data[box_idx][round_num].append(avg_rmse)\n        \n        averaged_data = {}\n        for box_idx in range(common_boxes):\n            averaged_data[box_idx] = []\n            for round_values in model_data[box_idx]:\n                if round_values:\n                    averaged_data[box_idx].append(np.mean(round_values))\n        \n        results[model] = averaged_data\n    \n    return results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59ef85",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# COMPUTE FUNCTIONS - Aggregate across pre-loaded sessions\n# =============================================================================\n\ndef compute_domain_results(all_sessions, metric_prefix=\"\"):\n    \"\"\"\n    Compute domain-level results from pre-loaded sessions.\n    Returns: {domain: {model: {metric: [list of values]}}}\n    \"\"\"\n    domain_results = {}\n    \n    for domain, session_list in all_sessions.items():\n        # Initialize domain storage\n        domain_results[domain] = {model: {metric: [] for metric in metrics} for model in models}\n        \n        # Extract results from each session\n        for session_data in session_list:\n            session_result = extract_domain_results(session_data, metric_prefix)\n            \n            for model in models:\n                if model in session_result:\n                    for metric in metrics:\n                        if metric in session_result[model]:\n                            domain_results[domain][model][metric].append(\n                                session_result[model][metric]\n                            )\n    \n    return domain_results\n\ndef compute_round_by_round(all_sessions, metric_prefix=\"\", n_jobs=32):\n    \"\"\"\n    Compute round-by-round aggregated results from pre-loaded sessions.\n    For each round, only averages sessions that actually reached that round.\n    Also tracks mean session end round per agent to find when agents finish.\n    Calculates quartiles for agent activity (100%, 75%, 25%, 10% active).\n    \"\"\"\n    all_session_list = []\n    for domain, session_list in all_sessions.items():\n        all_session_list.extend(session_list)\n    \n    total_sessions = len(all_session_list)\n    print(f\"Processing {total_sessions} sessions for round-by-round analysis...\")\n    \n    session_results = Parallel(n_jobs=n_jobs)(\n        delayed(extract_round_by_round)(session_data, metric_prefix)\n        for session_data in tqdm(all_session_list, desc=\"Extracting round-by-round\")\n    )\n    \n    # Filter out empty results\n    session_results = [res for res in session_results if res]\n    \n    # Collect values per round: {model: {metric: {round_num: [values from all sessions]}}}\n    round_data = {model: {metric: {} for metric in metrics} for model in models}\n    \n    # Track all session end rounds for each agent\n    agent_session_ends = {}\n    \n    # Get all unique agents\n    all_agents = set()\n    for session_data in all_session_list:\n        if 'agent_a' in session_data and 'agent_b' in session_data:\n            all_agents.add(session_data['agent_a'])\n            all_agents.add(session_data['agent_b'])\n    \n    total_agents = len(all_agents)\n    print(f\"Total unique agents: {total_agents}\")\n    \n    # Process each session\n    for idx, session_result in enumerate(session_results):\n        session_data = all_session_list[idx]\n        \n        # Get agents and when this session ended\n        if 'agent_a' not in session_data or 'agent_b' not in session_data:\n            continue\n        \n        agent_a = session_data['agent_a']\n        agent_b = session_data['agent_b']\n        \n        # Get the round when this session ended\n        session_df = session_data.get('session_df')\n        if session_df is None or 'Round' not in session_df.columns:\n            continue\n        \n        accept_row_idx = session_data.get('accept_row_idx', len(session_df) - 1)\n        \n        try:\n            end_round = int(session_df.iloc[accept_row_idx]['Round'])\n        except:\n            continue\n        \n        # Track all end rounds for each agent\n        if agent_a not in agent_session_ends:\n            agent_session_ends[agent_a] = []\n        if agent_b not in agent_session_ends:\n            agent_session_ends[agent_b] = []\n        \n        agent_session_ends[agent_a].append(end_round)\n        agent_session_ends[agent_b].append(end_round)\n        \n        for model in models:\n            if model not in session_result:\n                continue\n            for metric in metrics:\n                if metric not in session_result[model]:\n                    continue\n                # session_result[model][metric] is a dict: {round_num: value}\n                for round_num, value in session_result[model][metric].items():\n                    if pd.notna(value):  # Skip NaN values\n                        if round_num not in round_data[model][metric]:\n                            round_data[model][metric][round_num] = []\n                        round_data[model][metric][round_num].append(value)\n    \n    # Calculate mean end round for each agent\n    agent_mean_ends = {}\n    for agent, end_rounds in agent_session_ends.items():\n        if end_rounds:\n            agent_mean_ends[agent] = np.mean(end_rounds)\n    \n    # Get the sorted list of agent mean end rounds\n    sorted_mean_ends = sorted(agent_mean_ends.values())\n    \n    # Calculate quartile rounds based on agent count\n    # - 100% active (min): round where all agents still have sessions\n    # - 75% active (25th percentile): round where 75% of agents still have sessions  \n    # - 25% active (75th percentile): round where only 25% of agents still have sessions\n    # - 10% active (90th percentile): round where only 10% of agents still have sessions\n    last_full_round = int(min(sorted_mean_ends)) if sorted_mean_ends else 0\n    round_75_pct_active = int(np.percentile(sorted_mean_ends, 25)) if sorted_mean_ends else 0\n    round_25_pct_active = int(np.percentile(sorted_mean_ends, 75)) if sorted_mean_ends else 0\n    round_10_pct_active = int(np.percentile(sorted_mean_ends, 90)) if sorted_mean_ends else 0\n    \n    # Compute mean and std for each round\n    aggregated_results = {model: {metric: {'mean': [], 'std': []} for metric in metrics} for model in models}\n    \n    for model in models:\n        for metric in metrics:\n            if not round_data[model][metric]:\n                continue\n            \n            # Sort by round number\n            sorted_rounds = sorted(round_data[model][metric].keys())\n            \n            for round_num in sorted_rounds:\n                values = round_data[model][metric][round_num]\n                if values:\n                    aggregated_results[model][metric]['mean'].append(np.mean(values))\n                    aggregated_results[model][metric]['std'].append(np.std(values))\n    \n    # Add metadata about the rounds and quartiles\n    aggregated_results['_metadata'] = {\n        'last_full_round': last_full_round,\n        'round_75_pct_active': round_75_pct_active,\n        'round_25_pct_active': round_25_pct_active,\n        'round_10_pct_active': round_10_pct_active,\n        'total_agents': total_agents,\n        'agents_at_last_round': total_agents\n    }\n    \n    print(f\"Agent activity milestones:\")\n    print(f\"  - 100% agents active until round {last_full_round}\")\n    print(f\"  - 75% agents active until round {round_75_pct_active}\")\n    print(f\"  - 25% agents active until round {round_25_pct_active}\")\n    print(f\"  - 10% agents active until round {round_10_pct_active}\")\n    \n    return aggregated_results\n\ndef compute_agent_analysis(all_sessions, metric_prefix=\"\"):\n    \"\"\"\n    Compute per-agent opponent model performance from pre-loaded sessions.\n    Returns: {agent: {model: {metric: [list of values]}}}\n    \"\"\"\n    agent_results = {}\n    \n    # Process all sessions across all domains\n    for domain, session_list in all_sessions.items():\n        for session_data in session_list:\n            session_result = extract_opponent_analysis(session_data, metric_prefix=metric_prefix)\n            \n            for agent, models_data in session_result.items():\n                if agent not in agent_results:\n                    agent_results[agent] = {model: {metric: [] for metric in metrics} for model in models}\n                \n                for model, metrics_data in models_data.items():\n                    for metric, value in metrics_data.items():\n                        agent_results[agent][model][metric].append(value)\n    \n    return agent_results\n\ndef compute_box_rmse_for_domain(session_list, n_jobs=1):\n    \"\"\"\n    Compute box-specific RMSE for a SINGLE domain.\n    Safe version: Use n_jobs=1 if calling from another Parallel loop.\n    \"\"\"\n    \n    # PARALLEL extraction\n    # SAFETY: If outer loop is parallel, we MUST use serial extraction here\n    extraction_jobs = n_jobs if n_jobs is not None else 1\n    \n    if extraction_jobs > 1:\n        session_results = Parallel(n_jobs=extraction_jobs)(\n            delayed(extract_box_rmse)(session_data)\n            for session_data in session_list # Removed tqdm to avoid nested bars mess\n        )\n    else:\n        # Serial (Fastest when running inside another process)\n        session_results = [extract_box_rmse(s) for s in session_list]\n    \n    # Filter empty results (Crucial for avoiding crashes)\n    session_results = [r for r in session_results if r]\n    \n    # SEQUENTIAL aggregation\n    all_session_results = {model: {} for model in models}\n    \n    for session_result in session_results:\n        for model in models:\n            if model not in session_result:\n                continue\n            for box_idx, values in session_result[model].items():\n                if box_idx not in all_session_results[model]:\n                    all_session_results[model][box_idx] = []\n                all_session_results[model][box_idx].append(values)\n    \n    # Compute mean/std\n    aggregated_results = {}\n    for model in models:\n        aggregated_results[model] = {}\n        for box_idx, session_arrays in all_session_results[model].items():\n            if not session_arrays:\n                continue\n            max_rounds = max(len(arr) for arr in session_arrays)\n            means, stds = [], []\n            for r in range(max_rounds):\n                vals = [arr[r] for arr in session_arrays if r < len(arr)]\n                if vals:\n                    means.append(np.mean(vals))\n                    stds.append(np.std(vals))\n            aggregated_results[model][box_idx] = {'mean': means, 'std': stds}\n    \n    return aggregated_results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ca33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AGGREGATION HELPERS\n",
    "# =============================================================================\n",
    "\n",
    "def aggregate_results_for_boxplot(domain_results):\n",
    "    \"\"\"\n",
    "    Aggregate domain results for boxplot visualization.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Models x Metrics, where each cell contains a list of domain values\n",
    "    \"\"\"\n",
    "    overall_results = pd.DataFrame([[[] for _ in metrics] for _ in models], \n",
    "                                   index=models, columns=metrics)\n",
    "    \n",
    "    for domain, domain_data in domain_results.items():\n",
    "        for model in models:\n",
    "            if model in domain_data:\n",
    "                for metric in metrics:\n",
    "                    if metric in domain_data[model]:\n",
    "                        # domain_data[model][metric] is already a list of values from sessions\n",
    "                        # We extend the overall list with these values\n",
    "                        overall_results.loc[model, metric].extend(domain_data[model][metric])\n",
    "    \n",
    "    return overall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cda2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CACHING - Save/load sessions to avoid reloading\n",
    "# =============================================================================\n",
    "\n",
    "import pickle\n",
    "def load_sessions_cache(cache_path=\"session_cache.pkl\"):\n",
    "    \"\"\"\n",
    "    Load sessions from pickle cache if available.\n",
    "\n",
    "    Args:\n",
    "        cache_path: Path to the cache file\n",
    "\n",
    "    Returns:\n",
    "        all_sessions dict if cache exists, None otherwise\n",
    "    \"\"\"\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            all_sessions = pickle.load(f)\n",
    "\n",
    "        total_sessions = sum(len(sessions) for sessions in all_sessions.values())\n",
    "        print(f\"\u2713 Loaded {total_sessions} cached sessions from {cache_path}\")\n",
    "        return all_sessions\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d641c6b",
   "metadata": {},
   "source": [
    "\n",
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee482c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_metrics_subplots(overall_results, save_path=None):\n",
    "    \"\"\"\n",
    "    Creates boxplots for each metric showing distribution across all sessions.\n",
    "    \n",
    "    Uses Wong's colorblind-safe palette with consistent colors per model.\n",
    "    \n",
    "    Args:\n",
    "        overall_results: DataFrame with models as index and metrics as columns\n",
    "        save_path: Optional path to save the figure\n",
    "    \"\"\"\n",
    "    metrics = overall_results.columns\n",
    "    num_metrics = len(metrics)\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_metrics, figsize=(10 * num_metrics, 12))\n",
    "\n",
    "    if num_metrics == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        data = []\n",
    "        labels = []\n",
    "        colors = []\n",
    "\n",
    "        for model in overall_results.index:\n",
    "            values = overall_results.loc[model, metric]\n",
    "            if isinstance(values, list) and len(values) > 0:\n",
    "                data.append(values)\n",
    "                labels.append(\" \".join(model.split(\" \")[0:-2]))\n",
    "                colors.append(get_model_color(model))\n",
    "\n",
    "        bp = ax.boxplot(data, patch_artist=True, widths=0.6)\n",
    "        \n",
    "        # Color each box with the corresponding model color\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "            patch.set_linewidth(2)\n",
    "        \n",
    "        # Style the other elements with thicker lines\n",
    "        for element in ['whiskers', 'caps']:\n",
    "            plt.setp(bp[element], color='black', linewidth=2)\n",
    "        plt.setp(bp['medians'], color='black', linewidth=3)\n",
    "        plt.setp(bp['fliers'], markeredgecolor='black', markersize=8, alpha=0.5)\n",
    "        \n",
    "        # Larger, bolder title and labels\n",
    "        # Title removed per user request\n",
    "        ax.set_xlabel('Opponent Model', fontsize=22, fontweight='bold', labelpad=12)\n",
    "        ax.set_ylabel(metric, fontsize=22, fontweight='bold', labelpad=12)\n",
    "        \n",
    "        # Larger, bolder tick labels\n",
    "        ax.set_xticks(range(1, len(labels) + 1))\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=16, fontweight='bold')\n",
    "        ax.tick_params(axis='y', labelsize=16)\n",
    "        for label in ax.get_yticklabels():\n",
    "            label.set_fontweight('bold')\n",
    "        \n",
    "        ax.grid(True, alpha=0.3, axis='y', linewidth=1.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71204234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_round_by_round_performance(aggregated_results, save_path=None, figsize=(28, 16), marker_interval=3):\n",
    "    \"\"\"\n",
    "    Plot round-by-round performance for all metrics as line plots.\n",
    "    \n",
    "    Each subplot shows one metric with all opponent models as different lines.\n",
    "    Uses Wong's colorblind-safe palette for consistency across all visualizations.\n",
    "    Uses distinct geometric markers (circle, square, triangle, diamond, etc.) for each model.\n",
    "    Adds vertical lines showing agent activity milestones:\n",
    "      - Red solid: 100% agents active\n",
    "      - Orange dashed: 75% agents active\n",
    "      - Purple dotted: 25% agents active\n",
    "      - Green dash-dot: 10% agents active\n",
    "    \n",
    "    Args:\n",
    "        aggregated_results: Results from compute_round_by_round\n",
    "        save_path: Optional path to save the figure\n",
    "        figsize: Figure size tuple\n",
    "        marker_interval: Show markers every N rounds (default: 3)\n",
    "    \"\"\"\n",
    "    num_metrics = len(metrics)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Extract metadata about rounds and quartiles\n",
    "    metadata = aggregated_results.get('_metadata', {})\n",
    "    last_full_round = metadata.get('last_full_round', 0)\n",
    "    round_75_pct = metadata.get('round_75_pct_active', 0)\n",
    "    round_25_pct = metadata.get('round_25_pct_active', 0)\n",
    "    round_10_pct = metadata.get('round_10_pct_active', 0)\n",
    "    total_agents = metadata.get('total_agents', 0)\n",
    "    \n",
    "    # Store handles and labels for shared legend\n",
    "    handles = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        for model in models:\n",
    "            if metric in aggregated_results[model]:\n",
    "                mean_values = aggregated_results[model][metric]['mean'][1:]\n",
    "                std_values = aggregated_results[model][metric]['std'][1:]\n",
    "                \n",
    "                if mean_values:\n",
    "                    rounds = list(range(len(mean_values)))\n",
    "                    mean_array = np.array(mean_values)\n",
    "                    \n",
    "                    # Shorten model name for legend\n",
    "                    model_short = \" \".join(model.split(\" \")[0:-2])\n",
    "                    \n",
    "                    # Get consistent color and marker for this model\n",
    "                    color = get_model_color(model)\n",
    "                    marker = get_model_marker(model)\n",
    "                    \n",
    "                    # Plot line without markers first\n",
    "                    line, = ax.plot(rounds, mean_array, \n",
    "                           label=model_short, \n",
    "                           color=color, \n",
    "                           linewidth=1.5,\n",
    "                           alpha=0.9)\n",
    "                    \n",
    "                    # Add markers at sampled intervals\n",
    "                    sampled_rounds = rounds[::marker_interval]\n",
    "                    sampled_values = mean_array[::marker_interval]\n",
    "                    ax.scatter(sampled_rounds, sampled_values,\n",
    "                              marker=marker,\n",
    "                              color=color,\n",
    "                              s=100,  # marker size\n",
    "                              edgecolors='black',\n",
    "                              linewidths=1.0,\n",
    "                              zorder=5,\n",
    "                              alpha=0.9)\n",
    "                    \n",
    "                    # Collect handles and labels only from first subplot\n",
    "                    if idx == 0:\n",
    "                        # Create a combined handle for legend\n",
    "                        from matplotlib.lines import Line2D\n",
    "                        combined_handle = Line2D([0], [0], color=color, linewidth=1.5,\n",
    "                                                 marker=marker, markersize=10,\n",
    "                                                 markeredgecolor='black', markeredgewidth=1.0)\n",
    "                        handles.append(combined_handle)\n",
    "                        labels.append(model_short)\n",
    "        \n",
    "        # Add vertical lines for agent activity milestones\n",
    "        # 100% agents active (solid red)\n",
    "        if last_full_round > 0:\n",
    "            plot_position = last_full_round - 1  # Subtract 1 because we skip round 0\n",
    "            ax.axvline(x=plot_position, color='red', linestyle='-', \n",
    "                      linewidth=2.5, alpha=0.8, zorder=10)\n",
    "        \n",
    "        # 75% agents active (dashed orange)\n",
    "        if round_75_pct > 0:\n",
    "            plot_position = round_75_pct - 1\n",
    "            ax.axvline(x=plot_position, color='#FF8C00', linestyle='--', \n",
    "                      linewidth=2.5, alpha=0.8, zorder=10)\n",
    "        \n",
    "        # 25% agents active (dotted purple)\n",
    "        if round_25_pct > 0:\n",
    "            plot_position = round_25_pct - 1\n",
    "            ax.axvline(x=plot_position, color='#8B008B', linestyle=':', \n",
    "                      linewidth=2.5, alpha=0.8, zorder=10)\n",
    "        \n",
    "        # 10% agents active (dash-dot green)\n",
    "        if round_10_pct > 0:\n",
    "            plot_position = round_10_pct - 1\n",
    "            ax.axvline(x=plot_position, color='#006400', linestyle='-.', \n",
    "                      linewidth=2.5, alpha=0.8, zorder=10)\n",
    "        \n",
    "        # Larger, bolder labels and title\n",
    "        ax.set_xlabel('Round', fontsize=20, fontweight='bold', labelpad=10)\n",
    "        ax.set_ylabel(metric, fontsize=20, fontweight='bold', labelpad=10)\n",
    "        # Title removed per user request\n",
    "        \n",
    "        # Larger, bolder tick labels\n",
    "        ax.tick_params(axis='both', labelsize=16)\n",
    "        for label in ax.get_xticklabels():\n",
    "            label.set_fontweight('bold')\n",
    "        for label in ax.get_yticklabels():\n",
    "            label.set_fontweight('bold')\n",
    "        \n",
    "        ax.grid(True, alpha=0.3, linewidth=1.5)\n",
    "    \n",
    "    # Add vertical lines to legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    \n",
    "    if last_full_round > 0:\n",
    "        line_100 = Line2D([0], [0], color='red', linestyle='-', linewidth=2.5)\n",
    "        handles.append(line_100)\n",
    "        labels.append(f'100% active (round {last_full_round})')\n",
    "    \n",
    "    if round_75_pct > 0:\n",
    "        line_75 = Line2D([0], [0], color='#FF8C00', linestyle='--', linewidth=2.5)\n",
    "        handles.append(line_75)\n",
    "        labels.append(f'75% active (round {round_75_pct})')\n",
    "    \n",
    "    if round_25_pct > 0:\n",
    "        line_25 = Line2D([0], [0], color='#8B008B', linestyle=':', linewidth=2.5)\n",
    "        handles.append(line_25)\n",
    "        labels.append(f'25% active (round {round_25_pct})')\n",
    "    \n",
    "    if round_10_pct > 0:\n",
    "        line_10 = Line2D([0], [0], color='#006400', linestyle='-.', linewidth=2.5)\n",
    "        handles.append(line_10)\n",
    "        labels.append(f'10% active (round {round_10_pct})')\n",
    "    \n",
    "    # Add single large legend outside the plots\n",
    "    fig.legend(handles, labels, \n",
    "              loc='center left',\n",
    "              bbox_to_anchor=(1.0, 0.5),\n",
    "              fontsize=16,\n",
    "              frameon=True,\n",
    "              shadow=True,\n",
    "              framealpha=0.95,\n",
    "              edgecolor='black',\n",
    "              fancybox=True,\n",
    "              title='Opponent Model',\n",
    "              title_fontsize=18,\n",
    "              ncol=1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # Make room for the legend\n",
    "    plt.subplots_adjust(right=0.82)\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nm73bj11n9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_domain_heatmap(domain_results, metric='RMSE', figsize=(14, 10)):\n",
    "    \"\"\"\n",
    "    Creates a heatmap showing opponent model performance across different domains.\n",
    "    \n",
    "    Args:\n",
    "        domain_results: Dictionary from compute_domain_results()\n",
    "        metric: Which metric to visualize ('RMSE', 'Spearman', 'KendallTau', 'Pearson')\n",
    "        figsize: Figure size tuple\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib figure object\n",
    "    \"\"\"\n",
    "    # Get sorted domain names\n",
    "    domain_names = sorted(domain_results.keys())\n",
    "    \n",
    "    # Initialize matrix to store metric values\n",
    "    metric_matrix = np.zeros((len(models), len(domain_names)))\n",
    "    metric_matrix[:] = np.nan  # Start with NaN for missing data\n",
    "    \n",
    "    # Populate the matrix (compute mean of session values)\n",
    "    for j, domain in enumerate(domain_names):\n",
    "        for i, model in enumerate(models):\n",
    "            if model in domain_results[domain] and metric in domain_results[domain][model]:\n",
    "                values = domain_results[domain][model][metric]\n",
    "                if values:  # Check if list is not empty\n",
    "                    metric_matrix[i, j] = np.mean(values)\n",
    "    \n",
    "    # Calculate median for each model (row) and sort\n",
    "    row_medians = np.nanmedian(metric_matrix, axis=1)\n",
    "    \n",
    "    # Sort indices: ascending for RMSE (lower is better), descending for correlations (higher is better)\n",
    "    if metric == 'RMSE':\n",
    "        sorted_indices = np.argsort(row_medians)  # Ascending\n",
    "    else:\n",
    "        sorted_indices = np.argsort(row_medians)[::-1]  # Descending\n",
    "    \n",
    "    # Reorder matrix and labels\n",
    "    metric_matrix = metric_matrix[sorted_indices, :]\n",
    "    sorted_models = [models[i] for i in sorted_indices]\n",
    "    model_labels = [\" \".join(model.split(\" \")[0:-2]) for model in sorted_models]\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Choose colormap based on metric (lower is better for RMSE, higher is better for correlations)\n",
    "    if metric == 'RMSE':\n",
    "        cmap = 'rocket_r'  # Lower values are better (lighter color)\n",
    "    else:\n",
    "        cmap = 'rocket'  # Higher values are better (darker color)\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(metric_matrix, annot=True, fmt='.3f', cmap=cmap,\n",
    "                xticklabels=domain_names, yticklabels=model_labels,\n",
    "                ax=ax, cbar_kws={'label': metric},\n",
    "                mask=np.isnan(metric_matrix))  # Mask NaN values\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel('Domain', fontsize=12)\n",
    "    ax.set_ylabel('Opponent Model', fontsize=12)\n",
    "    # Title removed per user request\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_domain_heatmap_by_categories(domain_results, metric='RMSE', figsize=(32, 28)):\n",
    "    \"\"\"\n",
    "    Creates three beautiful heatmaps showing opponent model performance across domains,\n",
    "    grouped by domain characteristics: Size, Opposition, and Balance.\n",
    "    \n",
    "    Args:\n",
    "        domain_results: Dictionary from compute_domain_results()\n",
    "        metric: Which metric to visualize ('RMSE', 'Spearman', 'KendallTau', 'Pearson')\n",
    "        figsize: Figure size tuple for the entire figure\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib figure object\n",
    "    \"\"\"\n",
    "    # Create figure with 3 subplots (one per category)\n",
    "    fig, axes = plt.subplots(3, 1, figsize=figsize)\n",
    "    \n",
    "    categories = ['size', 'opposition', 'balance']\n",
    "    category_titles = ['Domain Size', 'Domain Opposition', 'Domain Balance']\n",
    "    \n",
    "    # Choose colormap - using more readable colormaps\n",
    "    if metric == 'RMSE':\n",
    "        # For RMSE: lower is better - use reversed colormap (light = good, dark = bad)\n",
    "        cmap = 'YlOrRd'  # Yellow (good) to Red (bad)\n",
    "    else:\n",
    "        # For correlations: higher is better - use sequential colormap (dark = good, light = bad)\n",
    "        cmap = 'YlGnBu'  # Yellow (bad) to Blue/Green (good)\n",
    "    \n",
    "    for idx, (category, cat_title) in enumerate(zip(categories, category_titles)):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Get domains sorted by this category\n",
    "        domain_names = get_domains_by_category(domain_results, category)\n",
    "        \n",
    "        if not domain_names:\n",
    "            ax.text(0.5, 0.5, f'No domain data for {cat_title}', \n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=20)\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        \n",
    "        # Initialize matrix to store metric values\n",
    "        metric_matrix = np.zeros((len(models), len(domain_names)))\n",
    "        metric_matrix[:] = np.nan\n",
    "        \n",
    "        # Populate the matrix\n",
    "        for j, domain in enumerate(domain_names):\n",
    "            for i, model in enumerate(models):\n",
    "                if model in domain_results[domain] and metric in domain_results[domain][model]:\n",
    "                    values = domain_results[domain][model][metric]\n",
    "                    if values:\n",
    "                        metric_matrix[i, j] = np.mean(values)\n",
    "        \n",
    "        # Calculate median for each model and sort\n",
    "        row_medians = np.nanmedian(metric_matrix, axis=1)\n",
    "        \n",
    "        if metric == 'RMSE':\n",
    "            sorted_indices = np.argsort(row_medians)  # Ascending (best first)\n",
    "        else:\n",
    "            sorted_indices = np.argsort(row_medians)[::-1]  # Descending (best first)\n",
    "        \n",
    "        # Reorder matrix and labels\n",
    "        metric_matrix = metric_matrix[sorted_indices, :]\n",
    "        sorted_models = [models[i] for i in sorted_indices]\n",
    "        model_labels = [\" \".join(model.split(\" \")[0:-2]) for model in sorted_models]\n",
    "        \n",
    "        # Create heatmap with larger fonts and cells\n",
    "        sns.heatmap(metric_matrix, annot=True, fmt='.3f', cmap=cmap,\n",
    "                    xticklabels=domain_names, yticklabels=model_labels,\n",
    "                    ax=ax, \n",
    "                    cbar_kws={'label': metric, 'shrink': 0.8},\n",
    "                    mask=np.isnan(metric_matrix),\n",
    "                    annot_kws={'fontsize': 11, 'weight': 'bold'},  # Bigger annotation font\n",
    "                    linewidths=0.5, linecolor='white',  # Cell borders for clarity\n",
    "                    square=False)  # Allow rectangular cells\n",
    "        \n",
    "        # Larger, bolder fonts for axis labels\n",
    "        ax.set_xlabel('Domain', fontsize=22, fontweight='bold', labelpad=12)\n",
    "        ax.set_ylabel('Opponent Model', fontsize=22, fontweight='bold', labelpad=12)\n",
    "        # Title removed per user request\n",
    "        \n",
    "        # Larger, bolder tick labels\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=16)\n",
    "        ax.tick_params(axis='y', rotation=0, labelsize=16)\n",
    "        \n",
    "        # Make tick labels bold\n",
    "        for label in ax.get_xticklabels():\n",
    "            label.set_fontweight('bold')\n",
    "        for label in ax.get_yticklabels():\n",
    "            label.set_fontweight('bold')\n",
    "        \n",
    "        plt.setp(ax.get_xticklabels(), ha='right')\n",
    "        \n",
    "        # Larger colorbar label\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.ax.tick_params(labelsize=14)\n",
    "        cbar.set_label(metric, fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # Suptitle removed per user request\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_opponent_histogram(agent_results, metric='RMSE', bins=None, figsize=(28, 24)):\n",
    "    \"\"\"\n",
    "    Creates stacked histograms showing opponent model performance distribution per agent.\n",
    "    \n",
    "    One histogram per opponent model, where each agent is a different color in the stack.\n",
    "    \n",
    "    Args:\n",
    "        agent_results: Dictionary from compute_agent_analysis()\n",
    "        metric: Which metric to visualize ('RMSE', 'Spearman', 'KendallTau', 'Pearson')\n",
    "        bins: Number of bins or bin edges (default: auto)\n",
    "        figsize: Figure size tuple\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib figure object\n",
    "    \"\"\"\n",
    "    # Get sorted agent names\n",
    "    agent_names = sorted(agent_results.keys())\n",
    "    \n",
    "    # Create a 3x3 grid for 7 models\n",
    "    fig, axes = plt.subplots(3, 3, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Generate colors for agents using tab10 colormap\n",
    "    agent_colors = plt.cm.tab10(np.linspace(0, 1, len(agent_names)))\n",
    "    \n",
    "    # Find global min/max for consistent binning across all models\n",
    "    all_values = []\n",
    "    for model in models:\n",
    "        for agent in agent_names:\n",
    "            if model in agent_results[agent] and metric in agent_results[agent][model]:\n",
    "                values = agent_results[agent][model][metric]\n",
    "                if values:\n",
    "                    all_values.extend(values)\n",
    "    \n",
    "    if not all_values:\n",
    "        print(\"No data to plot\")\n",
    "        return fig\n",
    "    \n",
    "    # Determine bins\n",
    "    if bins is None:\n",
    "        bins = 20  # Default number of bins\n",
    "    \n",
    "    global_min = np.min(all_values)\n",
    "    global_max = np.max(all_values)\n",
    "    bin_edges = np.linspace(global_min, global_max, bins + 1)\n",
    "    \n",
    "    # Plot each model\n",
    "    for model_idx, model in enumerate(models):\n",
    "        ax = axes[model_idx]\n",
    "        \n",
    "        # Collect data for each agent\n",
    "        agent_data = []\n",
    "        agent_labels = []\n",
    "        agent_color_list = []\n",
    "        \n",
    "        for agent_idx, agent in enumerate(agent_names):\n",
    "            if model in agent_results[agent] and metric in agent_results[agent][model]:\n",
    "                values = agent_results[agent][model][metric]\n",
    "                if values:\n",
    "                    agent_data.append(values)\n",
    "                    agent_labels.append(agent)\n",
    "                    agent_color_list.append(agent_colors[agent_idx])\n",
    "        \n",
    "        if agent_data:\n",
    "            # Create stacked histogram\n",
    "            ax.hist(agent_data, bins=bin_edges, stacked=True, \n",
    "                   color=agent_color_list, label=agent_labels,\n",
    "                   alpha=0.85, edgecolor='black', linewidth=0.5)\n",
    "            \n",
    "            # Labels and title\n",
    "            model_short = \" \".join(model.split(\" \")[0:-2])\n",
    "            # Title removed per user request\n",
    "            ax.set_xlabel(metric, fontsize=18, fontweight='bold', labelpad=10)\n",
    "            ax.set_ylabel('Count', fontsize=18, fontweight='bold', labelpad=10)\n",
    "            \n",
    "            # Larger, bolder tick labels\n",
    "            ax.tick_params(axis='both', labelsize=14)\n",
    "            for label in ax.get_xticklabels():\n",
    "                label.set_fontweight('bold')\n",
    "            for label in ax.get_yticklabels():\n",
    "                label.set_fontweight('bold')\n",
    "            \n",
    "            # Grid\n",
    "            ax.grid(True, alpha=0.3, linewidth=1.5, axis='y')\n",
    "            \n",
    "            # Legend only on first subplot\n",
    "            if model_idx == 0:\n",
    "                ax.legend(title='Agent', title_fontsize=14, \n",
    "                         fontsize=12, loc='upper right',\n",
    "                         frameon=True, shadow=True, framealpha=0.9)\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(len(models), 9):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    # Suptitle removed per user request\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_opponent_heatmap(agent_results, metric='RMSE', figsize=(18, 12)):\n",
    "    \"\"\"\n",
    "    Creates a beautiful heatmap showing opponent model performance for each agent.\n",
    "    \n",
    "    Args:\n",
    "        agent_results: Dictionary from compute_agent_analysis()\n",
    "        metric: Which metric to visualize ('RMSE', 'Spearman', 'KendallTau', 'Pearson')\n",
    "        figsize: Figure size tuple\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib figure object\n",
    "    \"\"\"\n",
    "    # Get sorted agent names\n",
    "    agent_names = sorted(agent_results.keys())\n",
    "    \n",
    "    # Initialize matrix to store metric values\n",
    "    metric_matrix = np.zeros((len(models), len(agent_names)))\n",
    "    metric_matrix[:] = np.nan  # Start with NaN for missing data\n",
    "    \n",
    "    # Populate the matrix (compute mean across all sessions for each agent-model pair)\n",
    "    for j, agent in enumerate(agent_names):\n",
    "        for i, model in enumerate(models):\n",
    "            if model in agent_results[agent] and metric in agent_results[agent][model]:\n",
    "                values = agent_results[agent][model][metric]\n",
    "                if values:\n",
    "                    metric_matrix[i, j] = np.mean(values)\n",
    "    \n",
    "    # Calculate median for each model (row) and sort\n",
    "    row_medians = np.nanmedian(metric_matrix, axis=1)\n",
    "    \n",
    "    # Sort indices: ascending for RMSE (lower is better), descending for correlations (higher is better)\n",
    "    if metric == 'RMSE':\n",
    "        sorted_indices = np.argsort(row_medians)  # Ascending\n",
    "    else:\n",
    "        sorted_indices = np.argsort(row_medians)[::-1]  # Descending\n",
    "    \n",
    "    # Reorder matrix and labels\n",
    "    metric_matrix = metric_matrix[sorted_indices, :]\n",
    "    sorted_models = [models[i] for i in sorted_indices]\n",
    "    model_labels = [\" \".join(model.split(\" \")[0:-2]) for model in sorted_models]\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Choose colormap - using more readable colormaps\n",
    "    if metric == 'RMSE':\n",
    "        cmap = 'YlOrRd'  # Yellow (good) to Red (bad)\n",
    "    else:\n",
    "        cmap = 'YlGnBu'  # Yellow (bad) to Blue/Green (good)\n",
    "    \n",
    "    # Create heatmap with larger fonts\n",
    "    sns.heatmap(metric_matrix, annot=True, fmt='.3f', cmap=cmap,\n",
    "                xticklabels=agent_names, yticklabels=model_labels,\n",
    "                ax=ax, \n",
    "                cbar_kws={'label': metric},\n",
    "                mask=np.isnan(metric_matrix),\n",
    "                annot_kws={'fontsize': 11, 'weight': 'bold'},\n",
    "                linewidths=0.5, linecolor='white')\n",
    "    \n",
    "    # Larger, bolder fonts for axis labels\n",
    "    ax.set_xlabel('Agent', fontsize=20, fontweight='bold', labelpad=12)\n",
    "    ax.set_ylabel('Opponent Model', fontsize=20, fontweight='bold', labelpad=12)\n",
    "    # Title removed per user request\n",
    "    \n",
    "    # Larger, bolder tick labels\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=15)\n",
    "    ax.tick_params(axis='y', rotation=0, labelsize=15)\n",
    "    \n",
    "    # Make tick labels bold\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_fontweight('bold')\n",
    "    for label in ax.get_yticklabels():\n",
    "        label.set_fontweight('bold')\n",
    "    \n",
    "    plt.setp(ax.get_xticklabels(), ha='right')\n",
    "    \n",
    "    # Larger colorbar\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=13)\n",
    "    cbar.set_label(metric, fontsize=17, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y7cb5i9yuhl",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pareto_box_rmse_by_model(box_results, domain=None, figsize=(28, 24)):\n",
    "    \"\"\"\n",
    "    Plot box-specific RMSE performance for all opponent models.\n",
    "\n",
    "    Creates a 3x3 grid (7 models + 2 empty).\n",
    "    Each subplot shows RMSE over rounds with one line per box index.\n",
    "    \n",
    "    Args:\n",
    "        box_results: Dict from compute_box_rmse_for_domain\n",
    "        domain: Domain name for title (optional)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for model_idx, model in enumerate(models):\n",
    "        ax = axes[model_idx]\n",
    "\n",
    "        if not box_results[model]:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "\n",
    "        # Get all box indices for this model\n",
    "        box_indices = sorted(box_results[model].keys())\n",
    "\n",
    "        # Define colors using viridis: early boxes (0) = purple, late boxes (max) = yellow\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(box_indices)))\n",
    "\n",
    "        # Plot line for each box\n",
    "        for color_idx, box_idx in enumerate(box_indices):\n",
    "            mean_values = box_results[model][box_idx]['mean'][1:]  # Skip round 0\n",
    "\n",
    "            if mean_values:\n",
    "                rounds = list(range(1, len(mean_values) + 1))\n",
    "                mean_array = np.array(mean_values)\n",
    "\n",
    "                ax.plot(rounds, mean_array,\n",
    "                       label=f'Box {box_idx}',\n",
    "                       color=colors[color_idx],\n",
    "                       linewidth=3,\n",
    "                       markersize=6,\n",
    "                       marker='o',\n",
    "                       alpha=0.85)\n",
    "\n",
    "        # Larger, bolder labels and title\n",
    "        ax.set_xlabel('Round', fontsize=18, fontweight='bold', labelpad=10)\n",
    "        ax.set_ylabel('RMSE', fontsize=18, fontweight='bold', labelpad=10)\n",
    "\n",
    "        model_short = \" \".join(model.split(\" \")[0:-2])\n",
    "        domain_str = f\" (Domain {domain})\" if domain else \"\"\n",
    "        # Title removed per user request\n",
    "        \n",
    "        # Larger, bolder tick labels\n",
    "        ax.tick_params(axis='both', labelsize=14)\n",
    "        for label in ax.get_xticklabels():\n",
    "            label.set_fontweight('bold')\n",
    "        for label in ax.get_yticklabels():\n",
    "            label.set_fontweight('bold')\n",
    "        \n",
    "        # Larger legend\n",
    "        ax.legend(title='Box Index', title_fontsize=13, loc='best', \n",
    "                 fontsize=12, ncol=2, frameon=True, shadow=True,\n",
    "                 framealpha=0.9, edgecolor='black')\n",
    "        \n",
    "        ax.grid(True, alpha=0.3, linewidth=1.5)\n",
    "\n",
    "    # Hide last 2 subplots\n",
    "    for idx in range(len(models), 9):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4e63d",
   "metadata": {},
   "source": [
    "# Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import friedmanchisquare, wilcoxon, ttest_rel, levene, ks_2samp\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_best_model_per_metric(ratings_df: pd.DataFrame) -> Dict[str, str]:\n",
    "    best_models = {}\n",
    "    for metric in ratings_df.columns:\n",
    "        mean_scores = ratings_df[metric].apply(lambda x: np.mean(np.array(x)))\n",
    "        best_models[metric] = mean_scores.idxmax()\n",
    "    return best_models\n",
    "\n",
    "\n",
    "def prepare_long_format_data(ratings_df: pd.DataFrame, metric: str) -> pd.DataFrame:\n",
    "    long_data_list = []\n",
    "    n_subjects = len(next(iter(ratings_df[metric])))\n",
    "    for subject_idx in range(n_subjects):\n",
    "        for model_name in ratings_df.index:\n",
    "            value = ratings_df.loc[model_name, metric][subject_idx]\n",
    "            long_data_list.append({\n",
    "                'Subject': subject_idx,\n",
    "                'Model': model_name,\n",
    "                'Value': value\n",
    "            })\n",
    "    return pd.DataFrame(long_data_list)\n",
    "\n",
    "\n",
    "def test_normality(data: np.ndarray) -> bool:\n",
    "    _, p_value = ks_2samp(data, np.random.normal(np.mean(data), np.std(data), len(data)))\n",
    "    return p_value >= 0.05\n",
    "\n",
    "\n",
    "def calculate_cohens_d(group1: np.ndarray, group2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Cohen's d effect size for two groups.\n",
    "    \n",
    "    Args:\n",
    "        group1: First group's data\n",
    "        group2: Second group's data\n",
    "        \n",
    "    Returns:\n",
    "        Cohen's d effect size value\n",
    "    \"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "    \n",
    "    # Pooled standard deviation\n",
    "    pooled_sd = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "    \n",
    "    # Cohen's d\n",
    "    d = (np.mean(group1) - np.mean(group2)) / pooled_sd\n",
    "    return d\n",
    "\n",
    "\n",
    "def perform_group_significance_test(ratings_df: pd.DataFrame, metric: str) -> Tuple[str, float]:\n",
    "    methods = list(ratings_df.index)\n",
    "\n",
    "    # If only two methods, use paired test\n",
    "    if len(methods) < 3:\n",
    "        arr1 = np.array(ratings_df.loc[methods[0], metric])\n",
    "        arr2 = np.array(ratings_df.loc[methods[1], metric])\n",
    "        norm1 = test_normality(arr1)\n",
    "        norm2 = test_normality(arr2)\n",
    "        if norm1 and norm2:\n",
    "            _, lev_p = levene(arr1, arr2)\n",
    "            if lev_p >= 0.05:\n",
    "                _, p_value = ttest_rel(arr1, arr2)\n",
    "                return \"Paired t-test\", p_value\n",
    "        _, p_value = wilcoxon(arr1, arr2)\n",
    "        return \"Wilcoxon Signed-Rank Test\", p_value\n",
    "\n",
    "    # For three or more, test normality\n",
    "    normal_tests = []\n",
    "    for method in methods:\n",
    "        data = np.array(ratings_df.loc[method, metric])\n",
    "        is_normal = test_normality(data)\n",
    "        normal_tests.append(is_normal)\n",
    "\n",
    "    # If all normal, try RM-ANOVA\n",
    "    if all(normal_tests):\n",
    "        long_data = prepare_long_format_data(ratings_df, metric)\n",
    "        try:\n",
    "            anova = AnovaRM(long_data, 'Value', 'Subject', within=['Model']).fit()\n",
    "            return \"Repeated Measures ANOVA\", anova.anova_table['Pr > F'][0]\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"RM ANOVA failed: {e}, falling back to Friedman test\")\n",
    "            \n",
    "    # Friedman fallback\n",
    "    friedman_data = [np.array(ratings_df.loc[m, metric]) for m in methods]\n",
    "    stat, p_value = friedmanchisquare(*friedman_data)\n",
    "    return \"Friedman Test\", p_value\n",
    "\n",
    "\n",
    "def perform_pairwise_comparison(\n",
    "    best_scores: List[float],\n",
    "    comparison_scores: List[float]\n",
    ") -> Tuple[str, float, float]:\n",
    "    best_array = np.array(best_scores)\n",
    "    comp_array = np.array(comparison_scores)\n",
    "    \n",
    "    # Calculate effect size\n",
    "    effect_size = calculate_cohens_d(best_array, comp_array)\n",
    "    \n",
    "    norm1 = test_normality(best_array)\n",
    "    norm2 = test_normality(comp_array)\n",
    "    if norm1 and norm2:\n",
    "        _, lev_p = levene(best_array, comp_array)\n",
    "        if lev_p >= 0.05:\n",
    "            _, p_value = ttest_rel(best_array, comp_array)\n",
    "            return \"Paired t-test\", p_value, effect_size\n",
    "    _, p_value = wilcoxon(best_array, comp_array)\n",
    "    return \"Wilcoxon Signed-Rank Test\", p_value, effect_size\n",
    "\n",
    "\n",
    "def interpret_effect_size(effect_size: float) -> str:\n",
    "    \"\"\"\n",
    "    Interpret Cohen's d effect size.\n",
    "    \n",
    "    Args:\n",
    "        effect_size: The calculated Cohen's d value\n",
    "        \n",
    "    Returns:\n",
    "        String interpretation of the effect size\n",
    "    \"\"\"\n",
    "    if abs(effect_size) < 0.2:\n",
    "        return \"negligible\"\n",
    "    elif abs(effect_size) < 0.5:\n",
    "        return \"small\"\n",
    "    elif abs(effect_size) < 0.8:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "\n",
    "\n",
    "def compare_models(ratings_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    comparisons_data: List[Dict] = []\n",
    "    best_models = get_best_model_per_metric(ratings_df)\n",
    "    for metric in ratings_df.columns:\n",
    "        best_model = best_models[metric]\n",
    "        group_test, group_p = perform_group_significance_test(ratings_df, metric)\n",
    "        if group_p < 0.05 and len(ratings_df.index) > 1:\n",
    "            for model in ratings_df.index:\n",
    "                if model == best_model:\n",
    "                    continue\n",
    "                try:\n",
    "                    test_used, p_val, effect_size = perform_pairwise_comparison(\n",
    "                        ratings_df.loc[best_model, metric],\n",
    "                        ratings_df.loc[model, metric]\n",
    "                    )\n",
    "                    effect_interpretation = interpret_effect_size(effect_size)\n",
    "                    \n",
    "                    comparisons_data.append({\n",
    "                        \"Metric\": metric,\n",
    "                        \"Group Test\": group_test,\n",
    "                        \"Group p\": group_p,\n",
    "                        \"Best Method\": best_model,\n",
    "                        \"Compared Method\": model,\n",
    "                        \"Pair Test\": test_used,\n",
    "                        \"Pair p\": p_val,\n",
    "                        \"Significant\": p_val < 0.05,\n",
    "                        \"Effect Size (Cohen's d)\": effect_size,\n",
    "                        \"Effect Interpretation\": effect_interpretation\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Comparison {best_model} vs {model} failed: {e}\")\n",
    "        else:\n",
    "            comparisons_data.append({\n",
    "                \"Metric\": metric,\n",
    "                \"Group Test\": group_test,\n",
    "                \"Group p\": group_p,\n",
    "                \"Best Method\": best_model,\n",
    "                \"Compared Method\": None,\n",
    "                \"Pair Test\": None,\n",
    "                \"Pair p\": None,\n",
    "                \"Significant\": False,\n",
    "                \"Effect Size (Cohen's d)\": None,\n",
    "                \"Effect Interpretation\": None\n",
    "            })\n",
    "    return pd.DataFrame(comparisons_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd6676",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c857b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MASTER SESSION LOADING - Load once, use everywhere\n",
    "# =============================================================================\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"session_cache.pkl\"):\n",
    "    all_sessions = load_sessions_cache(cache_path=\"session_cache.pkl\")\n",
    "else:\n",
    "    all_sessions = load_all_session_data(sessions_path, n_jobs=200)\n",
    "    # save_sessions(all_sessions, base_path=\"data/session_cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2858572",
   "metadata": {},
   "source": [
    "## Overall Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15691b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"RMSE\", \"Spearman\", \"KendallTau\", \"Pearson\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da8279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute domain results from cached sessions\n",
    "domain_results = compute_domain_results(all_sessions)\n",
    "\n",
    "# Aggregate for boxplot\n",
    "overall_results = aggregate_results_for_boxplot(domain_results)\n",
    "\n",
    "# Debug domain matching\n",
    "debug_domain_matching(domain_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_metrics_subplots(overall_results, save_path=PLOTS_DIR / \"overall_boxplots.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g0achwu3nov",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mean_std_table(overall_results):\n",
    "    \"\"\"\n",
    "    Create a table showing mean \u00b1 std for each model across all metrics.\n",
    "    \n",
    "    Args:\n",
    "        overall_results: DataFrame with models as index and metrics as columns,\n",
    "                        where each cell contains a list of values.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with formatted mean \u00b1 std strings\n",
    "    \"\"\"\n",
    "    # Create a new DataFrame for the summary\n",
    "    summary_data = {}\n",
    "    \n",
    "    for metric in overall_results.columns:\n",
    "        summary_data[metric] = []\n",
    "        for model in overall_results.index:\n",
    "            values = overall_results.loc[model, metric]\n",
    "            if isinstance(values, list) and len(values) > 0:\n",
    "                mean_val = np.mean(values)\n",
    "                std_val = np.std(values)\n",
    "                summary_data[metric].append(f\"{mean_val:.4f} \u00b1 {std_val:.4f}\")\n",
    "            else:\n",
    "                summary_data[metric].append(\"N/A\")\n",
    "    \n",
    "    # Create DataFrame with shortened model names\n",
    "    model_labels = [\" \".join(model.split(\" \")[0:-2]) for model in overall_results.index]\n",
    "    summary_df = pd.DataFrame(summary_data, index=model_labels)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Display mean \u00b1 std table for all metrics\n",
    "mean_std_table = create_mean_std_table(overall_results)\n",
    "print(\"Mean \u00b1 Std for Each Model Across All Metrics:\\n\")\n",
    "display(mean_std_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c6c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "significane_tests = overall_results.copy()\n",
    "\n",
    "def make_negative(x):\n",
    "    if isinstance(x, list):\n",
    "        return [-v for v in x]\n",
    "    return -x\n",
    "\n",
    "significane_tests['RMSE'] = significane_tests['RMSE'].apply(make_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a101cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(significane_tests).to_excel(\"overall_significance_test.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ny6yzq6lxt",
   "metadata": {},
   "source": [
    "### Domain Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pmtmrnptkh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Domain Heatmap by Categories (Size, Opposition, Balance)\n",
    "fig = plot_domain_heatmap_by_categories(domain_results, metric='Pearson')\n",
    "fig.savefig(PLOTS_DIR / \"overall_domain_heatmap_pearson.png\", dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / 'overall_domain_heatmap_pearson.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2t5774m7ejy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KendallTau Domain Heatmap by Categories (Size, Opposition, Balance)\n",
    "fig = plot_domain_heatmap_by_categories(domain_results, metric='KendallTau')\n",
    "fig.savefig(PLOTS_DIR / \"overall_domain_heatmap_kendalltau.png\", dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / 'overall_domain_heatmap_kendalltau.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddqqz4gu1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman Domain Heatmap by Categories (Size, Opposition, Balance)\n",
    "fig = plot_domain_heatmap_by_categories(domain_results, metric='Spearman')\n",
    "fig.savefig(PLOTS_DIR / \"overall_domain_heatmap_spearman.png\", dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / 'overall_domain_heatmap_spearman.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xq35fdjyie8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE Domain Heatmap by Categories (Size, Opposition, Balance)\n",
    "fig = plot_domain_heatmap_by_categories(domain_results, metric='RMSE')\n",
    "fig.savefig(PLOTS_DIR / \"overall_domain_heatmap_rmse.png\", dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / 'overall_domain_heatmap_rmse.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8pigah1iwvt",
   "metadata": {},
   "source": [
    "### Opponent Heatmaps (Per Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v5av4yif2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute opponent model performance per agent from cached sessions\n",
    "agent_results = compute_agent_analysis(all_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eocenfhnyjg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson by Agent\n",
    "fig = plot_opponent_heatmap(agent_results, metric='Pearson')\n",
    "fig.savefig(PLOTS_DIR / \"overall_agent_heatmap_pearson.png\", dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / 'overall_agent_heatmap_pearson.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uze6lm31ey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KendallTau by Agent\n",
    "fig = plot_opponent_heatmap(agent_results, metric='KendallTau')\n",
    "fig.savefig(PLOTS_DIR / \"overall_agent_heatmap_kendalltau.png\", dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / 'overall_agent_heatmap_kendalltau.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345uh1t45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman by Agent\n",
    "fig = plot_opponent_heatmap(agent_results, metric='Spearman')\n",
    "fig.savefig(PLOTS_DIR / \"overall_agent_heatmap_spearman.png\", dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / 'overall_agent_heatmap_spearman.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0r09wcehuqb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE by Agent\n",
    "fig = plot_opponent_heatmap(agent_results, metric='RMSE')\n",
    "fig.savefig(PLOTS_DIR / \"overall_agent_heatmap_rmse.png\", dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / 'overall_agent_heatmap_rmse.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6xrxvpuwt",
   "metadata": {},
   "source": [
    "### Round-by-Round Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y7xibv41b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute round-by-round results from cached sessions\n",
    "round_by_round_results = compute_round_by_round(all_sessions, n_jobs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot round-by-round performance for all metrics\n",
    "plot_round_by_round_performance(round_by_round_results, save_path=PLOTS_DIR / \"overall_round_by_round.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2q9graxjed",
   "metadata": {},
   "source": [
    "## Overall Pareto Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06da367",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"RMSE\", \"Spearman\", \"Kendall\", \"Pearson\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ts3bzfchx0m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute overall (Pareto) domain results from cached sessions\n",
    "pareto_domain_results = compute_domain_results(all_sessions, metric_prefix=\"Overall_\")\n",
    "\n",
    "# Aggregate for boxplot\n",
    "pareto_results = aggregate_results_for_boxplot(pareto_domain_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f7b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "significane_tests = pareto_results.copy()\n",
    "\n",
    "def make_negative(x):\n",
    "    if isinstance(x, list):\n",
    "        return [-v for v in x]\n",
    "    return -x\n",
    "\n",
    "significane_tests['RMSE'] = significane_tests['RMSE'].apply(make_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2596cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(significane_tests).to_excel(\"pareto_significance_test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y4jeuc6iwfi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for overall metrics\n",
    "plot_all_metrics_subplots(pareto_results, save_path=PLOTS_DIR / \"pareto_boxplots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pareto_domain_heatmaps_header",
   "metadata": {},
   "source": [
    "### Pareto Domain Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pareto_domain_heatmap_pearson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto Pearson Domain Heatmap by Categories (Size, Opposition, Balance)\n",
    "fig = plot_domain_heatmap_by_categories(pareto_domain_results, metric='Pearson')\n",
    "fig.savefig(PLOTS_DIR / \"pareto_domain_heatmap_pearson.png\", dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / 'pareto_domain_heatmap_pearson.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pareto_domain_heatmap_kendalltau",
   "metadata": {},
   "outputs": [],
   "source": "# Pareto Kendall Domain Heatmap by Categories (Size, Opposition, Balance)\nfig = plot_domain_heatmap_by_categories(pareto_domain_results, metric='Kendall')\nfig.savefig(PLOTS_DIR / \"pareto_domain_heatmap_kendall.png\", dpi=150, bbox_inches='tight')\nprint(f\"Saved: {PLOTS_DIR / 'pareto_domain_heatmap_kendall.png'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pareto_domain_heatmap_spearman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto Spearman Domain Heatmap by Categories (Size, Opposition, Balance)\n",
    "fig = plot_domain_heatmap_by_categories(pareto_domain_results, metric='Spearman')\n",
    "fig.savefig(PLOTS_DIR / \"pareto_domain_heatmap_spearman.png\", dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / 'pareto_domain_heatmap_spearman.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pareto_domain_heatmap_rmse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto RMSE Domain Heatmap by Categories (Size, Opposition, Balance)\n",
    "fig = plot_domain_heatmap_by_categories(pareto_domain_results, metric='RMSE')\n",
    "fig.savefig(PLOTS_DIR / \"pareto_domain_heatmap_rmse.png\", dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / 'pareto_domain_heatmap_rmse.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pareto_agent_heatmaps_header",
   "metadata": {},
   "source": [
    "### Pareto Opponent Heatmaps (Per Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pareto_agent_analysis",
   "metadata": {},
   "outputs": [],
   "source": "# Compute Pareto opponent model performance per agent from cached sessions\npareto_agent_results = compute_agent_analysis(all_sessions, metric_prefix=\"Overall_\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pareto_agent_heatmap_pearson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto Pearson by Agent\n",
    "fig = plot_opponent_heatmap(pareto_agent_results, metric='Pearson')\n",
    "fig.savefig(PLOTS_DIR / \"pareto_agent_heatmap_pearson.png\", dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / 'pareto_agent_heatmap_pearson.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pareto_agent_heatmap_kendalltau",
   "metadata": {},
   "outputs": [],
   "source": "# Pareto Kendall by Agent\nfig = plot_opponent_heatmap(pareto_agent_results, metric='Kendall')\nfig.savefig(PLOTS_DIR / \"pareto_agent_heatmap_kendall.png\", dpi=150, bbox_inches='tight')\nprint(f\"Saved: {PLOTS_DIR / 'pareto_agent_heatmap_kendall.png'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pareto_agent_heatmap_spearman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto Spearman by Agent\n",
    "fig = plot_opponent_heatmap(pareto_agent_results, metric='Spearman')\n",
    "fig.savefig(PLOTS_DIR / \"pareto_agent_heatmap_spearman.png\", dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / 'pareto_agent_heatmap_spearman.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pareto_agent_heatmap_rmse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto RMSE by Agent\n",
    "fig = plot_opponent_heatmap(pareto_agent_results, metric='RMSE')\n",
    "fig.savefig(PLOTS_DIR / \"pareto_agent_heatmap_rmse.png\", dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / 'pareto_agent_heatmap_rmse.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bdc85c",
   "metadata": {},
   "source": [
    "### Round-by-Round Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zjm1k5gm6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute overall (Pareto) round-by-round results from cached sessions\n",
    "overall_round_by_round_results = compute_round_by_round(all_sessions, metric_prefix=\"Overall_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c4ql1o0j6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Overall metrics by round (all 4 metrics)\n",
    "plot_round_by_round_performance(overall_round_by_round_results, save_path=PLOTS_DIR / \"pareto_round_by_round.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}